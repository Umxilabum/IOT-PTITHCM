{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import IterableDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from typing import List\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pylab as plt\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Intrusion_model(torch.nn.Module):\n",
    "    def __init__(self, num_classes: int) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(in_channels= 1, out_channels= 32, kernel_size= 3, groups= 1)\n",
    "        self.batch_norm1 = torch.nn.BatchNorm1d(num_features= 32)\n",
    "        self.drop_out1 = torch.nn.Dropout1d()\n",
    "        \n",
    "        self.conv2 = torch.nn.Conv1d(in_channels= 32, out_channels= 64, kernel_size= 3, groups= 1)\n",
    "        self.batch_norm2 = torch.nn.BatchNorm1d(num_features= 64)\n",
    "        self.avgpl2 = torch.nn.AvgPool1d(kernel_size= 3)\n",
    "        self.drop_out2 = torch.nn.Dropout1d()\n",
    "        \n",
    "        \n",
    "        self.conv3 = torch.nn.Conv1d(in_channels= 64, out_channels= 128, kernel_size= 3)\n",
    "        self.batch_norm3 = torch.nn.BatchNorm1d(num_features= 128)\n",
    "        self.drop_out3 = torch.nn.Dropout1d()\n",
    "\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.linear = torch.nn.Linear(in_features= 1536,out_features= num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.drop_out1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.avgpl2(x)\n",
    "        x = self.drop_out2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.drop_out3(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class MyDataset(IterableDataset):\n",
    "    \"\"\"Create dataset adopts to read multiple \n",
    "    files and also be able to load batch\"\"\"\n",
    "    def __init__(self, \n",
    "                 list_dirs: List[str],\n",
    "                 label2index: dict, \n",
    "                 num_cols: int, \n",
    "                 batch_size:int\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        self.list_dirs = list_dirs\n",
    "        self.label2index = label2index\n",
    "        self.num_cols = num_cols\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def mapping_normalized(self,column: pd.Series)-> pd.Series:\n",
    "        max_value = column.max()\n",
    "        min_value = column.min()\n",
    "        return (column - min_value)/(max_value - min_value)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for file_path in self.list_dirs:\n",
    "            file_data = pd.read_csv(file_path)\n",
    "            \n",
    "            # apply normalized along columns, x dtype pd.series\n",
    "            file_data = file_data.apply(lambda x: self.mapping_normalized(x) if x.name != \"label\" and \\\n",
    "                                        x.max()!=x.min() else x, axis= 0) \n",
    "            \n",
    "            labels =[self.label2index[str_label] for str_label in file_data[\"label\"].tolist()]\n",
    "            features = file_data.iloc[:, :self.num_cols-1].values.astype(np.float16)         \n",
    "            assert  np.any(np.isfinite(features)), f\"contains inf, {features}\"\n",
    "            \n",
    "            h, _ = features.shape\n",
    "            \n",
    "            # split into batchs\n",
    "            for batch_idx in range(0, h, self.batch_size):\n",
    "                batch_features = features[batch_idx : batch_idx+self.batch_size, :]\n",
    "                batch_labels = labels[batch_idx : batch_idx+1]\n",
    "                    \n",
    "                yield (torch.tensor(batch_features, dtype= torch.float32), \n",
    "                       torch.tensor(batch_labels, dtype= torch.long).squeeze())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num columns:  47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "87617it [05:21, 272.19it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'MyDataset' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 60\u001b[0m\n\u001b[0;32m     57\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     58\u001b[0m     loss_in_batch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 60\u001b[0m loss_in_batch \u001b[38;5;241m=\u001b[39m loss_in_batch\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[0;32m     61\u001b[0m total_train_loss\u001b[38;5;241m.\u001b[39mappend(loss_in_batch)\n\u001b[0;32m     63\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\AI\\myenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:474\u001b[0m, in \u001b[0;36mDataLoader.__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m    457\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;66;03m# NOTE [ IterableDataset and __len__ ]\u001b[39;00m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    472\u001b[0m \n\u001b[0;32m    473\u001b[0m         \u001b[38;5;66;03m# Cannot statically verify that dataset is Sized\u001b[39;00m\n\u001b[1;32m--> 474\u001b[0m         length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[assignment, arg-type]\u001b[39;00m\n\u001b[0;32m    475\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# IterableDataset doesn't allow custom sampler or batch_sampler\u001b[39;00m\n\u001b[0;32m    476\u001b[0m             \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ceil\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'MyDataset' has no len()"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    labels2indices = None\n",
    "    with open(\"labels2index.json\") as data: \n",
    "        labels2indices = json.load(data)\n",
    "\n",
    "    model = Intrusion_model(num_classes= len(labels2indices.keys()))\n",
    "    model.train()\n",
    "    \n",
    "    list_dirs = [path for path in glob.glob(\"cic_iot_data/*.csv\")]\n",
    "    \n",
    "    # train_size = int(len(list_dirs)*0.7)\n",
    "    train_ids = [0,2,4,1]\n",
    "    val_ids = [i for i in range(0, len(list_dirs))if i not in train_ids]\n",
    "    print(\"train ids: \", train_ids)\n",
    "    print(\"val ids: \", val_ids)\n",
    "    train_list_dirs = [list_dirs[i] for i in train_ids]\n",
    "    val_list_dirs = [list_dirs[i] for i in val_ids]\n",
    "    \n",
    "    _, w = pd.read_csv(list_dirs[0]).values.shape\n",
    "    print(\"Num columns: \",w)\n",
    "\n",
    "    train_dataset = MyDataset(list_dirs = train_list_dirs, \n",
    "                        label2index = labels2indices, \n",
    "                        num_cols = w, \n",
    "                        batch_size = 1)\n",
    "    \n",
    "    val_dataset = MyDataset(list_dirs = val_list_dirs, \n",
    "                        label2index = labels2indices, \n",
    "                        num_cols = w, \n",
    "                        batch_size = 1)\n",
    "        \n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                               batch_size = 8,\n",
    "                                               num_workers = 0)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, \n",
    "                                             batch_size = 8,\n",
    "                                             num_workers = 0)\n",
    "    \n",
    "    \n",
    "    optimizer = torch.optim.SGD(params= model.parameters(), lr = 0.01)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    total_train_loss = []\n",
    "    total_val_loss = []\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        loss_in_batch = 0.0\n",
    "        for ith, (features, labels) in tqdm(enumerate(train_loader)):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_in_batch += loss.item()\n",
    "        \n",
    "        loss_in_batch = loss_in_batch/(ith+1)\n",
    "        total_train_loss.append(loss_in_batch)\n",
    "\n",
    "        model.eval()\n",
    "        \n",
    "        val_loss_in_batch = 0.0\n",
    "        for val_ith, (val_features, val_labels) in tqdm(enumerate(val_loader)):\n",
    "            val_outputs = model(val_features)\n",
    "            val_loss = loss_fn(val_outputs, val_labels)\n",
    "            val_loss_in_batch += val_loss.item()\n",
    "        \n",
    "        val_loss_in_batch = val_loss_in_batch/(val_ith+1)\n",
    "        total_val_loss.append(val_loss_in_batch)\n",
    "\n",
    "        print(f\"Epoch: {epoch}, train loss: {loss_in_batch}, val loss {val_loss_in_batch}\\n\")\n",
    "        \n",
    "    plt.plot(total_train_loss, color = \"red\")\n",
    "    plt.plot(total_val_loss, color = \"green\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"train_val_losses.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
